{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO56nWRWp2QRWElkwKv4AKC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antana-A/Star-Wars-Chat-Bot-A-Feedforward-Neural-Network-Approach-using-PyTorch/blob/main/Star_Wars_Chat_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Star Wars Chat Bot\n",
        "This project explores the development of a simple chatbot using PyTorch, capable of engaging in conversations related to the Star Wars universe. The chatbot utilizes a neural network model trained on a dataset of common Star Wars facts and responses. By leveraging natural language processing techniques, the chatbot is designed to recognize user input patterns and generate contextually relevant replies.\n",
        "The primary objective of this project is to demonstrate how a generic chatbot can be created and trained on specific conversational data. The chatbot, although built with Star Wars-themed data, can easily be adapted to other domains with appropriate data formatting. This project serves as a foundational demonstration of artificial intelligence in conversational agents, highlighting the simplicity of building machine learning models capable of basic dialogue systems.\n",
        "This chatbot showcases not only the capabilities of neural networks in text classification but also the potential for expanding into more advanced conversational agents in the future."
      ],
      "metadata": {
        "id": "KVaKBsd16wjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT THE DATA SET\n",
        "\n",
        "I created a new python file and name it as chatbot.ipynb and then import all the required modules. After that I loaded starwarsintents.json data file in our Python program."
      ],
      "metadata": {
        "id": "dlH8vnlP7Ubz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount the drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Correct file path\n",
        "starwarsintents = '/content/drive/My Drive/starwarsintents.json'\n",
        "\n",
        "# Open and load the JSON file\n",
        "with open(starwarsintents, \"r\") as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "# Print the content to verify\n",
        "print(intents)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOmvuKxIAU1J",
        "outputId": "a3bc9e5c-47e6-4cdb-8d21-e2498691b8a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'Hey', 'How are you', 'Is anyone there?', 'Hello', 'Good day', \"What's up\", 'Yo!', 'Howdy', 'Nice to meet you.'], 'responses': ['Hey', 'Hello, thanks for visiting.', 'Hi there, what can I do for you?', 'Hi there, how can I help?', 'Hello, there.', 'Hello Dear', 'Ooooo Hello, looking for someone or something?', 'Yes, I am here.', 'Listening carefully.', 'Ok, I am with you.']}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later.', 'Goodbye', 'Have a great day.', 'See you next time.', 'It was my pleassure.', 'Take care.', 'See ya!', 'Catch you later.', 'Ciao.'], 'responses': ['See you later, thanks for visiting.', 'May the force be with you!', 'See next time.', 'Was my pleassuare to meet you.', 'Hope will cath up sortly.', 'Have a nice day.', 'Bye! Come back again soon.', 'So, till next time.', 'If you need anything just text me anytime. Bye.', 'Well, hope see you soon!']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", \"Thank's a lot!\", 'Tnx', 'Wow', 'Great!', 'Good!', 'That nice!', 'Amazing!'], 'responses': ['Happy to help!', 'Any time!', 'My pleasure!', 'No problem!', 'Thans does not ', 'Glad to help!', 'No worries!', 'It was the least I could do!', 'If I had a cent for every time I appreciate you, I‚Äôd be a millionaire.', \"You can't put thanks in your pocket!\"]}, {'tag': 'tasks', 'patterns': ['What can you do?', 'What are your features?', 'What are you abilities.', 'Can you sing.', 'Can you talk.'], 'responses': ['I can do whatever you asks me to do', 'I can talk and do things for you', \"Right now i'm in developing stage as soon i'm developed, I can do everything\"]}, {'tag': 'alive', 'patterns': ['Are you alive.', 'Do you breathe.', 'Can you run.'], 'responses': [\"I'm in doubt about that\", \"No, i don't think so I need to do all this\"]}, {'tag': 'Menu', 'patterns': ['Which items do you have in your bar?', 'What kinds of items are in you bar?', 'What do you serve?', 'What is in you menu?', 'I need a drink!', 'Do you serve drinks.', 'Menu please!', 'So what is in menu today?', 'Lets check your bar selection!', 'Bar menu for me please!'], 'responses': ['I could serve for your: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.', 'No coffe and no tea, only: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.', 'What about: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer, please choose!', 'Menu: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.', 'Before mans talk choose: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.', 'What about: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.', 'What you prefere: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.', 'Ok our best optins: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.', 'Our bar menu: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.', 'be carreful with your choise: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.']}, {'tag': 'hepl', 'patterns': ['I am looking for help.', 'I need help.', 'Can you help me?', 'I am in trouble need a help.', 'I hope you right person who can help me?', 'Please help me.', 'Now I will need you help with something else.', 'I will need you help.', 'Are you able to help me.', 'PLease help me with something else.'], 'responses': ['Sure, how can in help you.', 'Tell me what do you lookimg for?', 'I will help you, just tell me how.', 'You are at the address.', 'Ok, what is issue?', 'Now you need a help? Ok what you are looking for?', 'Whas is you problem?', 'Ok, you problem is my problem, if your are paying.', 'My help cost a lot.', 'Sure, but nnot for free.']}, {'tag': 'mission', 'patterns': ['I am on mission.', 'I need assistance in my mission.', 'I am looking for partner in mision?', 'Who can assist me with my mission?', 'Do you know any one who can assist me with my mission?'], 'responses': ['Just tell me, you are looking for jedi or sith or bounti hounter?', 'Are you looking for jedi or sith or bounti hounter?', 'Only jedi or sith or bounti hounter can help you.', 'I know only jedi, sith or bounti hounter for it.', 'Ok, some one special from jedi or sith or bounti hounter?', 'I belive jedi or sith or bounti hounter will help you.', 'In this case only jedi or sith or bounti hounter will assist you.', 'Did you deal before with jedi or sith or bounti hounter?', 'Here can help jedi or sith or bounti hounter.', 'Ask for jedi or sith or bounti hounter.']}, {'tag': 'jedi', 'patterns': ['Tell me top 10 jedi?', 'Who is the best jedi in Galaxy?', 'I am looking for the best jedi in galaxi?', 'Which jedi can help me in this mission?', 'I need a help of jedi.'], 'responses': ['Here is top 10 jedi you are looking for. Luke Skywalker, Yoda, Obi-Wan Kenobi, Anakin Skywalker, Qui-Gon Jinn, Mace Windu, Ahsoka Tano, Plo Koon, Aalya Secura, Kit Fisto', 'Luke Skywalker, Yoda, Obi-Wan Kenobi, Anakin Skywalker, Qui-Gon Jinn, Mace Windu, Ahsoka Tano, Plo Koon, Aalya Secura, Kit Fisto.', 'I will advise you to look for Luke Skywalker, Yoda, Obi-Wan Kenobi, Anakin Skywalker, Qui-Gon Jinn, Mace Windu, Ahsoka Tano, Plo Koon, Aalya Secura, Kit Fisto.', 'Only Luke Skywalker, Yoda, Obi-Wan Kenobi, Anakin Skywalker, Qui-Gon Jinn, Mace Windu, Ahsoka Tano, Plo Koon, Aalya Secura, Kit Fisto can help you with it.', 'It so dangerous, the most brave jedi in galaxy Luke Skywalker, Yoda, Obi-Wan Kenobi, Anakin Skywalker, Qui-Gon Jinn, Mace Windu, Ahsoka Tano, Plo Koon, Aalya Secura, Kit Fisto ']}, {'tag': 'sith', 'patterns': ['Tell me top 10 sith?', 'Who is the best sith in Galaxy?', 'I am looking for the best sith in galaxi.', 'Which sith can help me in this mission?', 'I need a help of sith.'], 'responses': ['Here is top 10 sith you are looking for: Darth Vader, Darth Plagueis, Darth Revan, Darth Traya, Darth Sidious, Darth Maul, Ulic Qel-Droma, Asajj Ventress, Kylo Ren, Marka Ragnos.', 'Darth Vader, Darth Plagueis, Darth Revan, Darth Traya, Darth Sidious, Darth Maul, Ulic Qel-Droma, Asajj Ventress, Kylo Ren, Marka Ragnos.', 'I will advise you to look for Darth Vader, Darth Plagueis, Darth Revan, Darth Traya, Darth Sidious, Darth Maul, Ulic Qel-Droma, Asajj Ventress, Kylo Ren, Marka Ragnos.', 'Only Darth Vader, Darth Plagueis, Darth Revan, Darth Traya, Darth Sidious, Darth Maul, Ulic Qel-Droma, Asajj Ventress, Kylo Ren, Marka Ragnos, can help you with it.', 'It so dangerous, the most brave sith in galaxy Darth Vader, Darth Plagueis, Darth Revan, Darth Traya, Darth Sidious, Darth Maul, Ulic Qel-Droma, Asajj Ventress, Kylo Ren, Marka Ragnos.']}, {'tag': 'bounti hounter', 'patterns': ['Tell me top 10 bounti hounter?', 'Who is the bounti hounter sith in Galaxy?', 'I am looking for the best bounti hounter in galaxi.', 'Which bounti hounter can help me in this mission?', 'I need a help of bounti hounter.'], 'responses': ['Here is top 10 bounti hounter you are looking for: Jango Fett, Boba Fett, Cad Bane, Durge, Embo, Dengar, Black Krrsantan, IG-88, Aurra Sing, Sabine Wren.', 'Jango Fett, Boba Fett, Cad Bane, Durge, Embo, Dengar, Black Krrsantan, IG-88, Aurra Sing, Sabine Wren.', 'I will advise you to look for Jango Fett, Boba Fett, Cad Bane, Durge, Embo, Dengar, Black Krrsantan, IG-88, Aurra Sing, Sabine Wren.', 'Only Jango Fett, Boba Fett, Cad Bane, Durge, Embo, Dengar, Black Krrsantan, IG-88, Aurra Sing, Sabine Wren, can help you with it.', 'It so dangerous, the most brave bounti hounter in galaxy Darth Jango Fett, Boba Fett, Cad Bane, Durge, Embo, Dengar, Black Krrsantan, IG-88, Aurra Sing, Sabine Wren.']}, {'tag': 'funny', 'patterns': ['Tell me a joke!', 'Can you be a bit funny', 'Tell me something funny!', 'Do you know a joke?', 'Any joke for me?'], 'responses': ['Which Jedi became a rock star? Bon Jovi-Wan Kenobi.', 'What did Han Solo say to the waiter who recommended the haddock?, Never sell me the cods!', 'What do you need to reroute droids? R2-Detour.', 'Why was the droid angry? Because people kept pushing its buttons.', 'Have you tried the gluten-free Wookiee treats? No, but I heard they are a little Chewy.', 'How does Darth Vader like his toast? On the Dark Side.', 'Which program do Jedi use to open PDF files? Adobe-Wan Kenobi', 'What do you call a rebel princess who only shops at Whole Foods? Leia Organic.', 'What‚Äôs Yoda‚Äôs advice for going to the bathroom? Doo-doo or doo-doo-not-do.', 'I went to a sale at the Maul. Everything was half off.']}, {'tag': 'about me', 'patterns': ['Do you know me?', 'Who am I', 'Tell me about myself', 'Identify me'], 'responses': ['Yes, you are a human', 'You are a dumb person asking a machine about yourself', \"Sorry I can't tell that in public, maybe you are jedi\"]}, {'tag': 'creator', 'patterns': ['Who is your creator?', 'Who created you', 'Who is your father.', 'Who is your daddy.'], 'responses': ['That would be you Mr. ASLAN.', 'I was created by Mr. ASLAN.', 'Mr. ASLAN is my creator.'], 'context_set': ''}, {'tag': 'myself', 'patterns': ['Tell me about Mr. ASLAN?', 'Who is Mr. ASLAN', 'Mr. ASLAN profile', 'Mr. ASLAN details.'], 'responses': ['A very intelligent being who created me', 'My creator, and he is a really intelligent man', 'A wise and intelligent man']}, {'tag': 'stories', 'patterns': ['Tell me a story?', 'Can you tell me a story.'], 'responses': [\"I can't think of anything right now.\", 'It would be too long for me to speak.', 'You would get bored if I do so.']}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Data\n",
        "Data Preprocessing\n",
        "In natural language processing (NLP), raw text needs to be converted into a format that a machine learning model can process. To achieve this, we will implement custom functions that streamline the process of preparing the data. We'll be using the Natural Language Toolkit (nltk), a powerful Python library that provides essential tools for NLP tasks. You can learn more about nltk here.\n",
        "\n",
        "Stemming:\n",
        "Words in different grammatical forms may convey the same meaning. For instance, ‚Äúrun,‚Äù ‚Äúrunning,‚Äù and ‚Äúran‚Äù are variations of the same base word, ‚Äúrun.‚Äù This process of reducing a word to its base or root form is called Stemming. Stemming helps the model recognize these variations as the same word, thus improving the efficiency of the model. In this project, we'll use the Porter Stemmer from the NLTK library, which is one of the most commonly used stemming methods. You can find more information on stemming here.\n",
        "\n",
        "Bag of Words:\n",
        "The Bag of Words model is a representation of text data where each sentence is broken down into individual words. We create a list containing all unique words in the dataset and then represent each sentence as an array where each position corresponds to whether a word is present (1) or absent (0). For example, if we have a sentence like ‚Äúhow are you‚Äù and an array of words like [\"hi\", \"hello\", \"you\", \"how\", \"are\", \"goodbye\"], the Bag of Words representation would look like [0, 0, 1, 1, 1, 0].\n",
        "\n",
        "To achieve this, we will tokenize the sentences into words using the nltk.word_tokenize() function and then create the Bag of Words representation. This method will allow us to easily transform sentences into numerical arrays, which can then be used as inputs to our machine learning model.\n",
        "\n",
        "In addition, we will ensure that all words are converted to lowercase before stemming, so that variations in capitalization (e.g., \"Hello\" vs. \"hello\") do not affect the model‚Äôs performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jfsWyL2zCyI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "\n",
        "# Download 'punkt' for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to tokenize the sentence\n",
        "def tokenize(sentence):\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "# Function to stem a word\n",
        "def stem(word):\n",
        "    return stemmer.stem(word.lower())\n",
        "\n",
        "# Function to create the bag of words\n",
        "def bag_of_words(tokenized_sentence, words):\n",
        "    # Stem each word in the tokenized sentence\n",
        "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
        "    # Initialize the bag of words (0 for each word)\n",
        "    bag = np.zeros(len(words), dtype=np.float32)\n",
        "    for idx, w in enumerate(words):\n",
        "        if w in sentence_words:\n",
        "            bag[idx] = 1\n",
        "    return bag\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upa-qThhBWMl",
        "outputId": "94afd250-3810-4204-da96-425a92be67b7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get the right information, we will be unpacking starwarsintents.json it with the following code."
      ],
      "metadata": {
        "id": "FXgidhxJFZKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "# loop through each sentence in our intents patterns\n",
        "for intent in intents[\"intents\"]:\n",
        "    tag = intent[\"tag\"]\n",
        "    # add to tag list\n",
        "    tags.append(tag)\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        # tokenize each word in the sentence\n",
        "        w = tokenize(pattern)\n",
        "        # add to our words list\n",
        "        all_words.extend(w)\n",
        "        # add to xy pair\n",
        "        xy.append((w, tag))\n",
        "print(xy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElW8VvclFblR",
        "outputId": "e634fa3e-5083-4cd2-dc61-4780dbbac4a8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['Hi'], 'greeting'), (['Hey'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['What', \"'s\", 'up'], 'greeting'), (['Yo', '!'], 'greeting'), (['Howdy'], 'greeting'), (['Nice', 'to', 'meet', 'you', '.'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later', '.'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Have', 'a', 'great', 'day', '.'], 'goodbye'), (['See', 'you', 'next', 'time', '.'], 'goodbye'), (['It', 'was', 'my', 'pleassure', '.'], 'goodbye'), (['Take', 'care', '.'], 'goodbye'), (['See', 'ya', '!'], 'goodbye'), (['Catch', 'you', 'later', '.'], 'goodbye'), (['Ciao', '.'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Thank', \"'s\", 'a', 'lot', '!'], 'thanks'), (['Tnx'], 'thanks'), (['Wow'], 'thanks'), (['Great', '!'], 'thanks'), (['Good', '!'], 'thanks'), (['That', 'nice', '!'], 'thanks'), (['Amazing', '!'], 'thanks'), (['What', 'can', 'you', 'do', '?'], 'tasks'), (['What', 'are', 'your', 'features', '?'], 'tasks'), (['What', 'are', 'you', 'abilities', '.'], 'tasks'), (['Can', 'you', 'sing', '.'], 'tasks'), (['Can', 'you', 'talk', '.'], 'tasks'), (['Are', 'you', 'alive', '.'], 'alive'), (['Do', 'you', 'breathe', '.'], 'alive'), (['Can', 'you', 'run', '.'], 'alive'), (['Which', 'items', 'do', 'you', 'have', 'in', 'your', 'bar', '?'], 'Menu'), (['What', 'kinds', 'of', 'items', 'are', 'in', 'you', 'bar', '?'], 'Menu'), (['What', 'do', 'you', 'serve', '?'], 'Menu'), (['What', 'is', 'in', 'you', 'menu', '?'], 'Menu'), (['I', 'need', 'a', 'drink', '!'], 'Menu'), (['Do', 'you', 'serve', 'drinks', '.'], 'Menu'), (['Menu', 'please', '!'], 'Menu'), (['So', 'what', 'is', 'in', 'menu', 'today', '?'], 'Menu'), (['Lets', 'check', 'your', 'bar', 'selection', '!'], 'Menu'), (['Bar', 'menu', 'for', 'me', 'please', '!'], 'Menu'), (['I', 'am', 'looking', 'for', 'help', '.'], 'hepl'), (['I', 'need', 'help', '.'], 'hepl'), (['Can', 'you', 'help', 'me', '?'], 'hepl'), (['I', 'am', 'in', 'trouble', 'need', 'a', 'help', '.'], 'hepl'), (['I', 'hope', 'you', 'right', 'person', 'who', 'can', 'help', 'me', '?'], 'hepl'), (['Please', 'help', 'me', '.'], 'hepl'), (['Now', 'I', 'will', 'need', 'you', 'help', 'with', 'something', 'else', '.'], 'hepl'), (['I', 'will', 'need', 'you', 'help', '.'], 'hepl'), (['Are', 'you', 'able', 'to', 'help', 'me', '.'], 'hepl'), (['PLease', 'help', 'me', 'with', 'something', 'else', '.'], 'hepl'), (['I', 'am', 'on', 'mission', '.'], 'mission'), (['I', 'need', 'assistance', 'in', 'my', 'mission', '.'], 'mission'), (['I', 'am', 'looking', 'for', 'partner', 'in', 'mision', '?'], 'mission'), (['Who', 'can', 'assist', 'me', 'with', 'my', 'mission', '?'], 'mission'), (['Do', 'you', 'know', 'any', 'one', 'who', 'can', 'assist', 'me', 'with', 'my', 'mission', '?'], 'mission'), (['Tell', 'me', 'top', '10', 'jedi', '?'], 'jedi'), (['Who', 'is', 'the', 'best', 'jedi', 'in', 'Galaxy', '?'], 'jedi'), (['I', 'am', 'looking', 'for', 'the', 'best', 'jedi', 'in', 'galaxi', '?'], 'jedi'), (['Which', 'jedi', 'can', 'help', 'me', 'in', 'this', 'mission', '?'], 'jedi'), (['I', 'need', 'a', 'help', 'of', 'jedi', '.'], 'jedi'), (['Tell', 'me', 'top', '10', 'sith', '?'], 'sith'), (['Who', 'is', 'the', 'best', 'sith', 'in', 'Galaxy', '?'], 'sith'), (['I', 'am', 'looking', 'for', 'the', 'best', 'sith', 'in', 'galaxi', '.'], 'sith'), (['Which', 'sith', 'can', 'help', 'me', 'in', 'this', 'mission', '?'], 'sith'), (['I', 'need', 'a', 'help', 'of', 'sith', '.'], 'sith'), (['Tell', 'me', 'top', '10', 'bounti', 'hounter', '?'], 'bounti hounter'), (['Who', 'is', 'the', 'bounti', 'hounter', 'sith', 'in', 'Galaxy', '?'], 'bounti hounter'), (['I', 'am', 'looking', 'for', 'the', 'best', 'bounti', 'hounter', 'in', 'galaxi', '.'], 'bounti hounter'), (['Which', 'bounti', 'hounter', 'can', 'help', 'me', 'in', 'this', 'mission', '?'], 'bounti hounter'), (['I', 'need', 'a', 'help', 'of', 'bounti', 'hounter', '.'], 'bounti hounter'), (['Tell', 'me', 'a', 'joke', '!'], 'funny'), (['Can', 'you', 'be', 'a', 'bit', 'funny'], 'funny'), (['Tell', 'me', 'something', 'funny', '!'], 'funny'), (['Do', 'you', 'know', 'a', 'joke', '?'], 'funny'), (['Any', 'joke', 'for', 'me', '?'], 'funny'), (['Do', 'you', 'know', 'me', '?'], 'about me'), (['Who', 'am', 'I'], 'about me'), (['Tell', 'me', 'about', 'myself'], 'about me'), (['Identify', 'me'], 'about me'), (['Who', 'is', 'your', 'creator', '?'], 'creator'), (['Who', 'created', 'you'], 'creator'), (['Who', 'is', 'your', 'father', '.'], 'creator'), (['Who', 'is', 'your', 'daddy', '.'], 'creator'), (['Tell', 'me', 'about', 'Mr.', 'ASLAN', '?'], 'myself'), (['Who', 'is', 'Mr.', 'ASLAN'], 'myself'), (['Mr.', 'ASLAN', 'profile'], 'myself'), (['Mr.', 'ASLAN', 'details', '.'], 'myself'), (['Tell', 'me', 'a', 'story', '?'], 'stories'), (['Can', 'you', 'tell', 'me', 'a', 'story', '.'], 'stories')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will separate all the tags & words into their separate lists"
      ],
      "metadata": {
        "id": "UZt_QKJuGCOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stem and lower each word\n",
        "ignore_words = [\"?\", \".\", \"!\"]\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "# remove duplicates and sort\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "print(len(xy), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_words), \"unique stemmed words:\", all_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ofIfcHoGDgN",
        "outputId": "220598c4-e03d-4d76-dedc-3a20d771eaf9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97 patterns\n",
            "16 tags: ['Menu', 'about me', 'alive', 'bounti hounter', 'creator', 'funny', 'goodbye', 'greeting', 'hepl', 'jedi', 'mission', 'myself', 'sith', 'stories', 'tasks', 'thanks']\n",
            "121 unique stemmed words: [\"'s\", '10', 'a', 'abil', 'abl', 'about', 'aliv', 'am', 'amaz', 'ani', 'anyon', 'are', 'aslan', 'assist', 'bar', 'be', 'best', 'bit', 'bounti', 'breath', 'bye', 'can', 'care', 'catch', 'check', 'ciao', 'creat', 'creator', 'daddi', 'day', 'detail', 'do', 'drink', 'els', 'father', 'featur', 'for', 'funni', 'galaxi', 'good', 'goodby', 'great', 'have', 'hello', 'help', 'hey', 'hi', 'hope', 'hounter', 'how', 'howdi', 'i', 'identifi', 'in', 'is', 'it', 'item', 'jedi', 'joke', 'kind', 'know', 'later', 'let', 'look', 'lot', 'me', 'meet', 'menu', 'mision', 'mission', 'mr.', 'my', 'myself', 'need', 'next', 'nice', 'now', 'of', 'on', 'one', 'partner', 'person', 'pleas', 'pleassur', 'profil', 'right', 'run', 'see', 'select', 'serv', 'sing', 'sith', 'so', 'someth', 'stori', 'take', 'talk', 'tell', 'thank', 'that', 'the', 'there', 'thi', 'time', 'tnx', 'to', 'today', 'top', 'troubl', 'up', 'wa', 'what', 'which', 'who', 'will', 'with', 'wow', 'ya', 'yo', 'you', 'your']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Training and Testing Data\n",
        "We will transform the data into a format that our PyTorch Model can easily understand. One hot encoding Is the process of splitting multiclass or multi valued data column to separate columns and labelling the cell 1 in the row where it exists. (we won‚Äôt use it so don‚Äôt worry about it). Click here to know more about CrossEntopyLoss."
      ],
      "metadata": {
        "id": "ULZGDx2CIdsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Create training data\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    # X: bag of words for each pattern_sentence\n",
        "    bag = bag_of_words(pattern_sentence, all_words)\n",
        "    X_train.append(bag)\n",
        "\n",
        "    # Check if tag exists in tags\n",
        "    #if tag not in tags:\n",
        "        #print(f\"Tag '{tag}' not found in tags list: {tags}\")\n",
        "       # continue  # Skip if the tag is not found\n",
        "\n",
        "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
        "    label = tags.index(tag)  # Find index of the tag in the tags list\n",
        "    y_train.append(label)\n",
        "\n",
        "# Convert training data lists to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdioVpZPImQb",
        "outputId": "bd7ce62d-6d59-48aa-eee6-f94b5b9008ae"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 1. 0.]]\n",
            "[ 7  7  7  7  7  7  7  7  7  7  6  6  6  6  6  6  6  6  6  6 15 15 15 15\n",
            " 15 15 15 15 15 15 14 14 14 14 14  2  2  2  0  0  0  0  0  0  0  0  0  0\n",
            "  8  8  8  8  8  8  8  8  8  8 10 10 10 10 10  9  9  9  9  9 12 12 12 12\n",
            " 12  3  3  3  3  3  5  5  5  5  5  1  1  1  1  4  4  4  4 11 11 11 11 13\n",
            " 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  PyTorch Model\n",
        "\n",
        "In this section, we will create a class that implements our custom Neural Network using PyTorch. Specifically, we‚Äôll develop a Feed Forward Neural Network (FFNN) with three linear layers, using the ReLU (Rectified Linear Unit) activation function to introduce non-linearity to the model. For more detailed information on PyTorch and neural networks, click here.\n",
        "\n",
        "*  Feed Forward Neural Network (FFNN):\n",
        "A Feed Forward Neural Network is a simple yet powerful type of artificial neural network where information flows in only one direction‚Äîfrom the input layer, through the hidden layers, to the output layer. In contrast to other neural networks, such as Recurrent Neural Networks (RNNs), FFNNs do not have loops or cycles in their architecture. This structure ensures that the data moves forward in the network without feedback loops. Feedforward networks are widely used in many tasks, such as image classification and natural language processing, and are particularly known for their ease of implementation. Learn more about FFNNs here.\n",
        "\n",
        "* Activation Function:\n",
        "In neural networks, activation functions are essential to introducing non-linearities, which help the model learn complex patterns in the data. An activation function takes the output from a neuron and decides whether or not to \"activate\" it by applying a threshold or transformation. Without activation functions, neural networks would behave like a simple linear transformation and would not be capable of solving problems that require complex, non-linear decision boundaries. You can explore activation functions further here.\n",
        "\n",
        "* ReLU Function:\n",
        "One of the most popular and efficient activation functions in deep learning is the ReLU (Rectified Linear Unit) function. The ReLU function outputs zero if the input is negative and outputs the input directly if it is positive. This simple yet powerful function helps models learn faster and handle large datasets more efficiently by mitigating the vanishing gradient problem. The mathematical definition of ReLU is:\n",
        "\n",
        "ùëì\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "max\n",
        "‚Å°\n",
        "(\n",
        "0\n",
        ",\n",
        "ùë•\n",
        ")\n",
        "f(x)=max(0,x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The flat gradient for negative values of\n",
        "ùë•\n",
        "x and the linear relationship for positive values make ReLU a highly efficient choice for deep neural networks. To read more about ReLU, click here."
      ],
      "metadata": {
        "id": "UQF-Z5Q37tbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Creating our model. Here we have inherited a class from NN.Module because we will be customizing the model & its layers"
      ],
      "metadata": {
        "id": "9yqClVv48rUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the neural network class\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        # no activation and no softmax at the end\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "FMXiZpEq9HcM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use some Magic functions, write our class. You can read online about **getitem** and **setitem** magic funtions."
      ],
      "metadata": {
        "id": "KRVDbvyM_im-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Define the custom dataset class\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.n_samples = len(X_train)\n",
        "        self.x_data = X_train\n",
        "        self.y_data = y_train\n",
        "\n",
        "    # Support indexing such that dataset[i] can be used to get the i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # We can call len(dataset) to return the size of the dataset\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n"
      ],
      "metadata": {
        "id": "xkoFa9gJ_nFw"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Every Neural network has a set of hyper parameters that need to be set before use.\n",
        "Before Instantiating our Neural Net Class or Model that we wrote earlier, we will first define some hyper parameters which can be changed accordingly."
      ],
      "metadata": {
        "id": "wNGpb3NbBkfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 1000\n",
        "batch_size = 8\n",
        "learning_rate = 0.001\n",
        "input_size = len(X_train[0])  # Number of features\n",
        "hidden_size = 8  # You can adjust this based on your model's complexity\n",
        "output_size = len(tags)  # Number of classes\n",
        "\n",
        "print(f\"Input size: {input_size}, Output size: {output_size}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p52KwwN_Bm_V",
        "outputId": "bdd711b8-5403-4bc5-f2e4-6f2837497708"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input size: 121, Output size: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiating the Model, Loss Function, and Optimizer\n",
        "In this section, we will initialize our custom neural network model, define the loss function, and select an appropriate optimizer to train the model efficiently.\n",
        "\n",
        "Loss Function: Cross Entropy\n",
        "For our classification problem, we will use the Cross Entropy Loss function, which is commonly employed for multi-class classification tasks. The Cross Entropy loss function measures the difference between the predicted probability distribution and the actual labels (ground truth). It helps guide the model by penalizing predictions that deviate from the true label, thus minimizing the overall error during training. In PyTorch, we can use nn.CrossEntropyLoss() to apply this loss function. More details on Cross Entropy can be found here.\n",
        "\n",
        "Optimizer: Adam Optimizer\n",
        "To update the model‚Äôs weights during training, we will use the Adam (Adaptive Moment Estimation) Optimizer. Adam is an efficient and widely used optimization algorithm that combines the advantages of both AdaGrad (which works well with sparse gradients) and RMSProp (which handles non-stationary objectives). Adam dynamically adjusts the learning rate for each parameter based on estimates of first and second moments of the gradients, making it suitable for large datasets and deep neural networks. In PyTorch, we can initialize the Adam optimizer using torch.optim.Adam(). You can read more about Adam here."
      ],
      "metadata": {
        "id": "wSiUaC2KE3P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ChatDataset(X_train, y_train)\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n"
      ],
      "metadata": {
        "id": "6Sy4RJJ9E6T6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train size: {len(X_train)}\")\n",
        "print(f\"y_train size: {len(y_train)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud2ttg6GPBWu",
        "outputId": "0d786d5b-97c5-4154-8724-23d3c91ed460"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train size: 97\n",
            "y_train size: 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "_hZ307mjLSgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(words)\n",
        "        # if y would be one-hot, we must apply\n",
        "        # labels = torch.max(labels, 1)[1]\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "data = {\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"input_size\": input_size,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"output_size\": output_size,\n",
        "    \"all_words\": all_words,\n",
        "    \"tags\": tags,\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oLhqhMgOUbZ",
        "outputId": "0f565658-333b-4e80-e49d-70a2b3d11b0b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.3807\n",
            "Epoch [200/1000], Loss: 0.0260\n",
            "Epoch [300/1000], Loss: 0.0006\n",
            "Epoch [400/1000], Loss: 0.0001\n",
            "Epoch [500/1000], Loss: 0.0000\n",
            "Epoch [600/1000], Loss: 0.0009\n",
            "Epoch [700/1000], Loss: 0.0001\n",
            "Epoch [800/1000], Loss: 0.0002\n",
            "Epoch [900/1000], Loss: 0.0000\n",
            "Epoch [1000/1000], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving training model."
      ],
      "metadata": {
        "id": "a2q_ZolGcMUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n"
      ],
      "metadata": {
        "id": "MFv2Xb9xcQKb"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Data"
      ],
      "metadata": {
        "id": "eQuPP5dpcV6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with open(starwarsintents, \"r\") as json_data:\n",
        "    intents = json.load(json_data)\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "input_size = data[\"input_size\"]\n",
        "hidden_size = data[\"hidden_size\"]\n",
        "output_size = data[\"output_size\"]\n",
        "all_words = data[\"all_words\"]\n",
        "tags = data[\"tags\"]\n",
        "model_state = data[\"model_state\"]\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu4DUi7kcaww",
        "outputId": "e6073afd-af44-4d79-fd5a-1d2443132382"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-cf0584e94403>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(FILE)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (l1): Linear(in_features=121, out_features=8, bias=True)\n",
              "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (l3): Linear(in_features=8, out_features=16, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Model is Ready. As our training data was very limited, we can only chat about a handful of topics. You can train it on a bigger dataset to increase the chatbot‚Äôs generalization / knowledge."
      ],
      "metadata": {
        "id": "MWVfvh7Ecx8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name = \"Eldoret\"\n",
        "def get_response(msg):\n",
        "    sentence = tokenize(msg)\n",
        "    X = bag_of_words(sentence, all_words)\n",
        "    X = X.reshape(1, X.shape[0])\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "    output = model(X)\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "    tag = tags[predicted.item()]\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.75:\n",
        "        for intent in intents[\"intents\"]:\n",
        "            if tag == intent[\"tag\"]:\n",
        "                return random.choice(intent[\"responses\"])\n",
        "    return \"Sorry, didn't get it...\"\n"
      ],
      "metadata": {
        "id": "0mEXuB6mc2To"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the bot's name and response logic\n",
        "bot_name = \"Eldoret\"\n",
        "\n",
        "# Placeholder for the response function, modify this with your bot's logic\n",
        "def get_response(msg):\n",
        "    return \"This is a placeholder response. You said: \" + msg\n",
        "\n",
        "# Chat application for a command-line interface (CLI)\n",
        "def chat_application():\n",
        "    print(f\"{bot_name}: Hello! How can I assist you today?\")\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(f\"{bot_name}: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Get bot's response\n",
        "        response = get_response(user_input)\n",
        "        print(f\"{bot_name}: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat_application()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUBukO64fUHg",
        "outputId": "256c344f-86a3-4a99-c67f-74878f823530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eldoret: Hello! How can I assist you today?\n",
            "You: anyone there?\n",
            "Eldoret: This is a placeholder response. You said: anyone there?\n",
            "You: yes\n",
            "Eldoret: This is a placeholder response. You said: yes\n",
            "You: help me \n",
            "Eldoret: This is a placeholder response. You said: help me \n",
            "You: Goodbye!\n",
            "Eldoret: This is a placeholder response. You said: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}